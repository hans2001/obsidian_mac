## 1. Why fuse pgvector + Millisearch?

- **pgvector (semantic search)**  
    Finds text chunks byÂ **meaning**Â (via embeddings). Example:
    - Query: â€œregulations about artificial intelligenceâ€
    - Match: â€œEU AI Act draftâ€ (no overlapping keywords, but conceptually related).
        
- **Millisearch (keyword search)**  
    Finds text chunks byÂ **exact words / phrases / filters**. Example:
    - Query: â€œAI regulationâ€
    - Match: â€œAI regulation in Chinaâ€ (because â€œAIâ€ and â€œregulationâ€ literally appear).
        

ğŸ‘‰ Neither is perfect alone:
- Semantic can miss exact matches if embeddings are fuzzy.
- Keyword can miss synonyms / paraphrases.

So weÂ **combine (fuse) results**Â from both â†’ hybrid retrieval.

---

## 2. What goes into both indexes?

Yes â€” the sameÂ **text chunks**Â (e.g. 300â€“800 tokens each).
- **pgvector**Â stores â†’Â `(chunk_text, embedding, metadata)`
- **Millisearch**Â stores â†’Â `(chunk_text, metadata)`
    
That way:
- pgvector givesÂ **semantic similarity scores**.
- Millisearch givesÂ **lexical ranks**Â (position in ranked list).
    
## 3. What is RRF (Reciprocal Rank Fusion)?

RRF =Â **Reciprocal Rank Fusion**.  
Itâ€™s a simple way to merge ranked lists from different retrievers (like pgvector + Millisearch).

Formula:
score(d)=âˆ‘râˆˆretrievers1k+rankr(d)score(d)=râˆˆretrieversâˆ‘â€‹k+rankrâ€‹(d)1â€‹
- `rank_r(d)`Â = position of documentÂ `d`Â in retrieverÂ `r`â€™s result list.
- `k`Â = constant (e.g. 60, prevents low-ranked docs from exploding).
- If a document appears in both lists, its scores add up â†’ gets boosted.

    ![[Screenshot 2025-09-03 at 11.45.15 AM.png]]

**Example**Â (k=60):
Query: â€œAI regulationâ€

- **Millisearch results**:
    1. â€œAI regulation in Chinaâ€ â†’ rank 1 â†’ score = 1/(60+1) = 0.01639
    2. â€œAI regulation EU Actâ€ â†’ rank 2 â†’ score = 1/(60+2) = 0.01613
        
- **pgvector results**:
    1. â€œEU AI Act draftâ€ â†’ rank 1 â†’ score = 0.01639
    2. â€œChina AI lawâ€ â†’ rank 2 â†’ score = 0.01613
        

Fusion:
- â€œEU AI Act draftâ€ (pg rank 1 only) = 0.01639
- â€œAI regulation in Chinaâ€ (meili rank 1 only) = 0.01639
- If something shows up inÂ **both lists**, its score doubles â†’ moves up.
    

ğŸ‘‰ This way,Â **overlapping hits get rewarded**, but unique hits from each retriever still have a chance.

---

## 4. End-to-End Flow

Hereâ€™s the whole pipeline:

### (A) Ingestion

1. Document â†’ chunk into passages (e.g., 500 tokens each).
    
2. For each chunk:
    
    - ComputeÂ **embedding**Â (OpenAI/HuggingFace).
        
    - Insert intoÂ **pgvector table**.
        
    - Insert intoÂ **Millisearch index**Â (text + metadata).
        

---

### (B) Query

User asks:Â _â€œWhat are AI regulations in Europe?â€_

1. **Millisearch search**
    
    - Input:Â `"AI regulations Europe"`
        
    - Output: ranked list by keyword match (`doc_id, chunk_text, rank`).
        
2. **pgvector search**
    
    - Input: embedding of query
        
    - Output: ranked list by cosine similarity (`doc_id, chunk_text, similarity`).
        

---

### (C) Fusion (RRF)

1. Normalize results into a common format:Â `(doc_id, rank, score)`.
    
2. Apply RRF:
    
    - Each doc gets a fused score from both lists.
        
    - If a doc appears in both â†’ boosted.
        
    - Sort by fused score.
        
---

### (D) Context Assembly

1. Pick top-k chunks (e.g. 6).
    
2. Concatenate into aÂ **context string**.
    
3. Pass into the LLM prompt:
    
    `Question: What are AI regulations in Europe? Context: [chunk1] [chunk2] ...`
    

---

### (E) LLM Answer

The LLM (e.g. GPT-4, Claude, etc.) generates a grounded answer using those chunks.

---

## 5. Visual Flow

 `â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ Document Store â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ chunks  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ pgvector DB  â”‚         â”‚ Millisearch DB â”‚  â”‚ (semantic)   â”‚         â”‚ (lexical+facets)â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚                           â”‚   cosine sim                   keyword match         â”‚                           â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â–¼           Reciprocal Rank Fusion                    â”‚             Top-k merged chunks                    â”‚               LLM Prompt                    â”‚                    â–¼              Grounded Answer`